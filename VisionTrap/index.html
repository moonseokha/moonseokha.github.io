<!DOCTYPE html>
<html lang="en">
<head>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    <meta charset="UTF-8">
    <title>VisionTrap</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1, user-scalable=no">
    <meta name="keywords" content="VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions">
    <meta name="author" content="....">
    <link rel="stylesheet" href="stylesheet.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const carousel = document.querySelector('.carousel-inner');
            const carouselItems = document.querySelectorAll('.carousel-item');
            const prevButton = document.querySelector('.carousel-control.prev');
            const nextButton = document.querySelector('.carousel-control.next');
            let currentIndex = 0;

            function showSlide(index) {
                if (index >= carouselItems.length) {
                    currentIndex = 0;
                } else if (index < 0) {
                    currentIndex = carouselItems.length - 1;
                } else {
                    currentIndex = index;
                }
                const offset = -currentIndex * 100;
                carousel.style.transform = `translateX(${offset}%)`;
            }

            function showNextSlide() {
                showSlide(currentIndex + 1);
            }

            function showPrevSlide() {
                showSlide(currentIndex - 1);
            }

            nextButton.addEventListener('click', showNextSlide);
            prevButton.addEventListener('click', showPrevSlide);

            setInterval(showNextSlide, 3000); 
        });
    </script>
</head>
<body>
<div class="header">
    <a href="https://sites.google.com/view/seokhamoon" target="_blank">Seokha Moon</a>
</div>
<div class="title">
    <h1><strong>VisionTrap</strong></h1>
    <div class="h1_5">Vision-Augmented Trajectory Prediction Guided by Textual Descriptions</div>    
    <br>
    <div class="authors">
        <center>
            <div class="author">
                <h3>Seokha&nbsp;Moon<sup>1</sup>&emsp; Hyun&nbsp;Woo<sup>1</sup>&emsp; Hongbeen&nbsp;Park<sup>1</sup>&emsp; Haeji&nbsp;Jeong<sup>1</sup>&emsp; Reza&nbsp;Mahjourian<sup>2</sup>&emsp; Hyung-gun&nbsp;Chi<sup>3</sup>&emsp; Hyerin&nbsp;Lim<sup>4</sup>&emsp; Sangpil&nbsp;Kim<sup>1</sup>&emsp; Jinkyu&nbsp;Kim<sup>1</sup></h3>
            </div>
        </center>
    </div>
    <div class="affiliations">
        <center>
            <p class="affiliation"><h3><sup>1</sup> Korea University&emsp; <sup>2</sup> UT Austin&emsp; <sup>3</sup> Purdue University &emsp; <sup>4</sup> Hyundai Motor Company</h3></p>
            <!-- <p class="affiliation"><h3><sup>*</sup>Corresponding Author</h3></p>  -->
            <p class="affiliation"><h3>Author's email: shmoon96@korea.ac.kr</h3></p> 
        </center>
    </div>  
    <center>
        <h2>ECCV 2024</h2>
    </center>
    <div class="code">
        <center>
            <a class="paper-btn" href="https://arxiv.org/abs/2407.12345">
                <span class="material-icons"> description </span>
                Paper
            </a>
            <a class="paper-btn" href="https://drive.google.com/file/d/1v_M_OuLnDzRo2uXyOrDfHNHbtoIcR3RA/">
                <span class="material-icons"> folder </span>
                Dataset
            </a>
        </center>
    </div>
</div>
<div class="container">
    <div class="sections-container">
        <div class="section">
            <p>Our research introduces <strong>VisionTrap</strong>, a novel method that significantly enhances trajectory prediction for autonomous vehicles by integrating visual cues from surround-view cameras and textual descriptions generated by Vision-Language Models. Additionally, we release the nuScenes-Text dataset, which augments the nuScenes dataset with rich textual descriptions to support further research.</p>
        </div>
        <div class="section">
            <h2 class="section-title">Abstract</h2>
            <div class="center-img">
                <img class="content" width="80%" height="80%" src="./assets/teaser.png"/>
            </div>
            <p>In the realm of autonomous driving, accurately predicting the future trajectories of road agents is crucial for ensuring safety and efficiency. Traditional trajectory prediction methods primarily rely on past trajectories and high-definition (HD) maps. While these inputs provide valuable information, they often miss out on essential contextual cues such as the intentions of pedestrians, road conditions, and dynamic interactions between agents.</p>
            <h2>Why Vision?</h2>
            <ul>
                <li><strong>Contextual Understanding:</strong> Surround-view cameras capture rich visual data that includes human gestures, gazes, and road conditions. These visual cues provide critical context that can significantly influence an agent’s behavior.</li>
                <li><strong>Real-time Insights:</strong> Visual inputs allow the model to understand and react to real-time changes in the environment, such as sudden movements of pedestrians or changes in traffic signals.</li>
            </ul>
            <h2>Why Textual Descriptions?</h2>
            <ul>
                <li><strong>Semantic Guidance:</strong> Textual descriptions generated by Vision-Language Models (VLMs) offer high-level semantic information that can guide the model’s learning process. These descriptions can highlight important aspects of the scene, such as “a pedestrian is carrying stacked items and is expected to remain stationary.”</li>
                <li><strong>Enhanced Supervision:</strong> By refining these textual descriptions with a Large Language Model (LLM), we provide clear and precise guidance to the model, improving its ability to learn relevant features from the visual data.</li>
            </ul>
        </div>
        <div class="section">
            <h2 class="section-title">Method</h2>
            <p>In our approach, we integrate visual and textual data to enhance trajectory prediction.</p>
            <ol>
                <li><strong>Per-agent State Encoder:</strong> Processes each agent’s past trajectories and attributes, encoding them into context features.</li>
                <li><strong>Visual Semantic Encoder:</strong> Integrates multi-view images and map data into a unified BEV feature, capturing crucial visual context. The Scene-Agent Interaction module considers the environment and agent interaction.</li>
                <li><strong>Text-driven Guidance Module:</strong> Uses textual descriptions to supervise the model, aligning visual features with semantic information through contrastive learning. (Only for training)</li>
                <li><strong>Trajectory Decoder:</strong> Predicts future positions of agents using enriched state embeddings, ensuring accurate trajectory prediction.</li>
            </ol>
            <p>This comprehensive methodology leverages both visual and textual cues to significantly improve the accuracy and reliability of trajectory predictions in autonomous driving.</p>
            <div class="center-img">
                <figure>
                    <img class="content" width="100%" height="100%" src="./assets/main.png"/>
                    </figure>
                </div>
        </div>
        <div class="section">
            <h2 class="section-title">Qualitative Results</h2>
            <p>We demonstrate the effectiveness of VisionTrap by comparing trajectory predictions with and without the Visual Semantic Encoder and Text-driven Guidance Module. The examples below show how incorporating visual and textual data significantly improves prediction accuracy</p>
            <div class="center-img">
                <figure>
                    <img class="content" width="100%" height="100%" src="./assets/QualitativeResults.png"/>
                </figure>
            </div>
        </div>
            <div class="section">
                <h2 class="section-title">nuScenes-Text Dataset</h2>
                <p>The nuScenes-Text dataset enriches the nuScenes dataset with detailed annotations for every object in each frame, providing three versions of descriptions from surround camera views. We removed location-specific information such as 'left', 'right' or 'away from the ego car' to prevent confusion and refined descriptions using a LLM for clarity. These annotations capture diverse semantic details, such as agent behaviors, semantic features, and environmental conditions.</p>
                <div class="carousel">
                    <div class="carousel-inner">
                        <div class="carousel-item active">
                            <img src="./assets/dataset1.png" alt="Slide 1">
                            <div class="carousel-caption">Examples of our generated textual descriptions</div>
                        </div>
                        <div class="carousel-item">
                            <img src="./assets/dataset2.png" alt="Slide 2">
                            <div class="carousel-caption">Example of captions for objects in ego-centric surround-view images from a single scene</div>
                        </div>
                        <div class="carousel-item">
                            <img src="./assets/dataset3.png" alt="Slide 3">
                            <div class="carousel-caption">Textual descriptions of unique scenarios in dataset 1</div>
                        </div>
                        <div class="carousel-item">
                            <img src="./assets/dataset4.png" alt="Slide 4">
                            <div class="carousel-caption">Textual descriptions of unique scenarios in dataset 2</div>
                        </div>
                    </div>
                    <div class="carousel-controls">
                        <button class="carousel-control prev">❮</button>
                        <button class="carousel-control next">❯</button>
                    </div>
                </div>
            </div>

                <div class="section">
                    <h2 class="section-title">nuScenes-Text Dataset Architecture</h2>
                    <div class="center-img">
                        <img class="content" width="70%" height="70%" src="./assets/dataset_architecture.png"/>
                    </div>
        
                    </div>

          <br><br><br>
            <div class="row">
                <div class="col-xs-12 text-left">
                    <h3>BibTeX</h3>
                    <p><strong>If you use our code or data, please cite:</strong></p>
                    <pre>
                <code>
@article{moon2024visiontrap,
title={VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions},
author={Moon, Seokha and Woo, Hyun and Park, Hongbeen and Jung, Haeji and Mahjourian, Reza and Chi, Hyung-gun and Lim, Hyerin and Kim, Sangpil and Kim, Jinkyu},
journal={arXiv preprint arXiv:2407.12345},
year={2024}
}
            </code>
            </pre>
        </div>
            </div>
</script>

</body></html>
