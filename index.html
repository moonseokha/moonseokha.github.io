<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Seokha Moon </title> <meta name="author" content="Seokha Moon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/arge.png?69eb5ea6c710ad022e9d2e6b5d86beb4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://moonseokha.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Seokha Moon </h1> <p class="desc"><i class="fa-solid fa-envelope"></i> shmoon96[at]korea[dot]ac[dot]kr</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?8bb36e4e22d5aa4d1998ca9261f792b1" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hi!😀 I’m a Ph.D. Student at Korea University, Vision &amp; AI lab (Advisor: Prof. <a href="https://visionai.korea.ac.kr/" rel="external nofollow noopener" target="_blank">Jinkyu Kim</a>). I earned a Bachelor’s degree in Computer Science from Yonsei University.</p> <p>My research interests lie in the field of Autonomous Robots 🤖 and Autonomous Driving 🚗. Recently, I have been focusing on camera-based perception tasks in autonomous driving, including 3D detection and occupancy prediction. I also explored trajectory prediction with an emphasis on modeling interactions between agents, using vision-driven text guidance as supervision to inform the model of relevant contextual cues needed to understand each agent’s situation.</p> <p>Currently, I am particularly interested in Vision-Language Navigation for autonomous robots and End-to-End frameworks for autonomous driving.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Apr 01, 2025</th> <td> 💼📄 Our paper “Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction” has been released on arXiv! The work was done during my internship at NAVER LABS. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 02, 2024</th> <td> 💼 Started a research internship at <a href="https://www.naverlabs.com/" rel="external nofollow noopener" target="_blank">NAVER LABS</a>, focusing on real-time 3D occupancy prediction for autonomous driving. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 06, 2024</th> <td> 📄✨ Our paper “Who Should Have Been Focused: Transferring Attention-based Knowledge from Future Observations for Trajectory Prediction” has been accepted to ICPR 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 04, 2024</th> <td> 📄✨ Our paper “VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions” has been accepted to ECCV 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 29, 2024</th> <td> 📄✨ Our paper “Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection” has been accepted to ICRA 2024! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/STREAMOCC.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="STREAMOCC.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="streamocc" class="col-sm-8"> <div class="title">Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction</div> <div class="author"> <em>Seokha Moon</em>, Janghyun Baek, Giseop Kim, Jinkyu Kim, and Sunwook Choi </div> <div class="periodical"> <em>Arxiv Preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2407.12345" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="3D%20occupancy%20prediction%20has%20emerged%20as%20a%20key%20perception%20task%20for%20autonomous%20driving,%20as%20it%20reconstructs%203D%20environments%20to%20provide%20a%20comprehensive%20scene%20understanding.%20%0A%20%20%20%20Recent%20studies%20focus%20on%20integrating%20spatiotemporal%20information%20obtained%20from%20past%20observations%20to%20improve%20prediction%20accuracy,%20%0A%20%20%20%20using%20a%20multi-frame%20fusion%20approach%20that%20processes%20multiple%20past%20frames%20together.%20%0A%20%20%20%20However,%20these%20methods%20struggle%20with%20a%20trade-off%20between%20efficiency%20and%20accuracy,%20%0A%20%20%20%20which%20significantly%20limits%20their%20practicality.%20%0A%20%20%20%20To%20mitigate%20this%20trade-off,%20we%20propose%20StreamOcc,%20a%20novel%20framework%20that%20aggregates%20spatio-temporal%20information%20in%20a%20stream-based%20manner.%20%0A%20%20%20%20StreamOcc%20consists%20of%20two%20key%20components:%20(i)%20Stream-based%20Voxel%20Aggregation,%20which%20effectively%20accumulates%20past%20observations%20while%20minimizing%20computational%20costs,%20%0A%20%20%20%20and%20(ii)%20Query-guided%20Aggregation,%20which%20recurrently%20aggregates%20instance-level%20features%20of%20dynamic%20objects%20into%20corresponding%20voxel%20features,%20refining%20fine-grained%20details%20of%20dynamic%20objects.%20%0A%20%20%20%20Experiments%20on%20the%20Occ3D-nuScenes%20dataset%20show%20that%20StreamOcc%20achieves%20state-of-the-art%20performance%20in%20real-time%20settings,%20while%20reducing%20memory%20usage%20by%20more%20than%2050%%20compared%20to%20previous%20methods." class="btn btn-sm z-depth-0" role="button">Abs</a> </div> </div> </div></li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/VISIONTRAP.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="VISIONTRAP.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="visiontrap" class="col-sm-8"> <div class="title">VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions</div> <div class="author"> <em>Seokha Moon</em>, Hyun Woo, Hongbeen Park, Haeji Jung, Reza Mahjourian, Hyung-gun Chi, Hyerin Lim, Sangpil Kim, and Jinkyu Kim </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2407.12345" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="Predicting%20future%20trajectories%20for%20other%20road%20agents%20is%20an%20essential%20task%20for%20autonomous%20vehicles.%20%20Established%20trajectory%20prediction%20methods%20primarily%20use%20agent%20tracks%20generated%20by%20a%20detection%20and%20tracking%20system%20and%20HD%20map%20as%20inputs.%0AIn%20this%20work,%20we%20propose%20a%20novel%20method%20that%20also%20incorporates%20visual%20input%20from%20surround-view%20cameras,%20allowing%20the%20model%20to%20utilize%20visual%20cues%20such%20as%20human%20gazes%20and%20gestures,%20road%20conditions,%20vehicle%20turn%20signals,%20etc,%20which%20are%20typically%20hidden%20from%20the%20model%20in%20prior%20methods.%20Furthermore,%20we%20use%20textual%20descriptions%20generated%20by%20a%20Vision-Language%20Model%20(VLM)%20and%20refined%20by%20a%20Large%20Language%20Model%20(LLM)%20as%20supervision%20during%20training%20to%20guide%20the%20model%20on%20what%20to%20learn%20from%20the%20input%20data.%0ADespite%20using%20these%20extra%20inputs,%20our%20method%20achieves%20a%20latency%20of%2053%20ms,%20making%20it%20feasible%20for%20real-time%20processing,%20which%20is%20significantly%20faster%20than%20that%20of%20previous%20single-agent%20prediction%20methods%20with%20similar%20performance.%0AOur%20experiments%20show%20that%20both%20the%20visual%20inputs%20and%20the%20textual%20descriptions%20contribute%20to%20improvements%20in%20trajectory%20prediction%20performance,%20and%20our%20qualitative%20analysis%20highlights%20how%20the%20model%20is%20able%20to%20exploit%20these%20additional%20inputs.%20%0ALastly,%20in%20this%20work%20we%20create%20and%20release%20the%20nuScenes-Text%20dataset,%20which%20augments%20the%20established%20nuScenes%20dataset%20with%20rich%20textual%20annotations%20for%20every%20scene,%20demonstrating%20the%20positive%20impact%20of%20utilizing%20VLM%20on%20trajectory%20prediction.%20Our%20project%20page%20is%20at%20here(https://moonseokha.github.io/VisionTrap)" class="btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://moonseokha.github.io/VisionTrap/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> </div> </div></li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICPR</abbr> <figure> <picture> <img src="/assets/img/publication_preview/WHO.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="WHO.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="who" class="col-sm-8"> <div class="title">Who Should Have Been Focused: Transferring Attention-based Knowledge from Future Observations for Trajectory Prediction</div> <div class="author"> <em>Seokha Moon</em>, Kyuhwan Yeon, Hayoung Kim, Seong-Gyun Jeong, and Jinkyu Kim </div> <div class="periodical"> <em>International Conference on Pattern Recognition (ICPR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://link.springer.com/chapter/10.1007/978-3-031-78447-7_23" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="Accurately%20predicting%20the%20future%20behaviors%20of%20dynamic%20agents%20is%20crucial%20for%20the%20safe%20navigation%20of%20autonomous%20robotics.%20%0A%20%20%20%20However,%20it%20remains%20challenging%20because%20the%20agent%E2%80%99s%20intentions%20(or%20goals)%20are%20inherently%20uncertain,%20%0A%20%20%20%20making%20it%20difficult%20to%20accurately%20predict%20their%20future%20poses%20given%20the%20past%20and%20current%20observations.%0A%20%20%20%20To%20reduce%20such%20agents%E2%80%99%20intentional%20uncertainties,%20we%20utilize%20a%20student-teacher%20learning%20approach%20where%20the%20teacher%20model%20leverages%20other%20agents%E2%80%99%20ground-truth%20future%20observations%20to%20know%20which%20agents%20should%20be%20attended%20to%20for%20the%20final%20verdict.%20%0A%20%20%20%20Such%20attentional%20knowledge%20is%20then%20transferred%20into%20the%20student%20model%20by%20forcing%20%0A%20%20%20%20it%20to%20mimic%20the%20teacher%20model%E2%80%99s%20agent-wise%20attention.%20Further,%20we%20propose%20a%20Lane-guided%20Attention%20Module%20(LAM)%20to%20refine%20predicted%20trajectories%20with%20local%20information,%20%0A%20%20%20%20bridging%20the%20information%20gap%20between%20the%20teacher%20and%20student%20models.%20We%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20model%20with%20a%20large-scale%20Argoverse%20motion%20forecasting%20dataset,%20%0A%20%20%20%20achieving%20matched%20or%20better%20performance%20than%20the%20current%20state-of-the-art%20approaches.%20Plus,%20our%20model%20generates%20more%20human-intuitive%20trajectories,%20e.g.,%20avoiding%20collisions%20with%20other%20agents,%20keeping%20its%20lane,%20or%20%0A%20%20%20%20considering%20relations%20with%20other%20agents." class="btn btn-sm z-depth-0" role="button">Abs</a> </div> </div> </div></li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> <figure> <picture> <img src="/assets/img/publication_preview/DAP.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DAP.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dap" class="col-sm-8"> <div class="title">Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection</div> <div class="author"> <em>Seokha Moon</em>, Hongbeen Park, Jungphil Kwon, Jaekoo Lee, and Jinkyu Kim </div> <div class="periodical"> <em>International Conference on Robotics and Automation (ICRA)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2404.01580" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="In%20autonomous%20driving%20and%20robotics,%20there%20is%20a%20growing%20interest%20in%20utilizing%20short-term%20historical%20data%20to%20enhance%20multi-camera%203D%20object%20detection,%20leveraging%20the%20continuous%20and%20correlated%20nature%20of%20input%20video%20streams.%20Recent%20work%20has%20focused%20on%20spatially%20aligning%20BEV-based%20features%20over%20timesteps.%20However,%20this%20is%20often%20limited%20as%20its%20gain%20does%20not%20scale%20well%20with%20long-term%20past%20observations.%20To%20address%20this,%20we%20advocate%20for%20supervising%20a%20model%20to%20predict%20objects%E2%80%99%20poses%20given%20past%20observations,%20thus%20explicitly%20guiding%20to%20learn%20objects%E2%80%99%20temporal%20cues.%20To%20this%20end,%20we%20propose%20a%20model%20called%20DAP%20(Detection%20After%20Prediction),%20consisting%20of%20a%20two-branch%20network:%20(i)%20a%20branch%20responsible%20for%20forecasting%20the%20current%20objects%E2%80%99%20poses%20given%20past%20observations%20and%20(ii)%20another%20branch%20that%20detects%20objects%20based%20on%20the%20current%20and%20past%20observations.%20The%20features%20predicting%20the%20current%20objects%20from%20branch%20(i)%20is%20fused%20into%20branch%20(ii)%20to%20transfer%20predictive%20knowledge.%20We%20conduct%20extensive%20experiments%20with%20the%20large-scale%20nuScenes%20datasets,%20and%20we%20observe%20that%20utilizing%20such%20predictive%20information%20significantly%20improves%20the%20overall%20detection%20performance.%20Our%20model%20can%20be%20used%20plug-and-play,%20showing%20consistent%20performance%20gain." class="btn btn-sm z-depth-0" role="button">Abs</a> </div> </div> </div></li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WACV</abbr> <figure> <picture> <img src="/assets/img/publication_preview/BEVMAP.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="BEVMAP.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BEVMap" class="col-sm-8"> <div class="title">BEVMap: Map-Aware BEV Modeling for 3D Perception</div> <div class="author"> Mincheol Chang, <em>Seokha Moon</em>, Reza Mahjourian, and Jinkyu Kim </div> <div class="periodical"> <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openaccess.thecvf.com/content/WACV2024/html/Chang_BEVMap_Map-Aware_BEV_Modeling_for_3D_Perception_WACV_2024_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="In%20autonomous%20driving%20applications,%20there%20is%20a%20strong%20preference%20for%20modeling%20the%20world%20in%20Bird%E2%80%99s-Eye%20View%20(BEV),%20as%20it%20leads%20to%20improved%20accuracy%20and%20performance.%20BEV%20features%20are%20widely%20used%20in%20perception%20tasks%20since%20they%20allow%20fusing%20information%20from%20multiple%20views%20in%20an%20efficient%20manner.%20However,%20BEV%20features%20generated%20from%20camera%20images%20are%20prone%20to%20be%20imprecise%20due%20to%20the%20difficulty%20of%20estimating%20depth%20in%20the%20perspective%20view.%20Improper%20placement%20of%20BEV%20features%20limits%20the%20accuracy%20of%20downstream%20tasks.%20We%20introduce%20a%20method%20for%20incorporating%20map%20information%20to%20improve%20perspective%20depth%20estimation%20from%202D%20camera%20images%20and%20thereby%20producing%20geometrically-%20and%20semantically-robust%20BEV%20features.%20We%20show%20that%20augmenting%20the%20camera%20images%20with%20the%20BEV%20map%20and%20map-to-camera%20projections%20can%20compensate%20for%20the%20depth%20uncertainty%20and%20enrich%20camera-only%20BEV%20features%20with%20road%20contexts.%20Experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20our%20method%20outperforms%20previous%20approaches%20using%20only%20camera%20images%20in%20segmentation%20and%20detection%20tasks." class="btn btn-sm z-depth-0" role="button">Abs</a> </div> </div> </div></li> <li><div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">BMVC</abbr> <figure> <picture> <img src="/assets/img/publication_preview/ORA3D.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ORA3D.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ora3d" class="col-sm-8"> <div class="title">ORA3D: Overlap Region Aware Multi-view 3D Object Detection </div> <div class="author"> Wonseok Roh, Gyusam Chang, <em>Seokha Moon</em>, Giljoo Nam, Chanyoung Kim, Younghyun Kim, Jinkyu Kim, and Sangpil Kim </div> <div class="periodical"> <em>British Machine Vision Conference (BMVC)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2207.00865" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="Current%20multi-view%203D%20object%20detection%20methods%20often%20fail%20to%20detect%20objects%20in%20the%20overlap%20region%20properly,%20and%20the%20networks%E2%80%99%20understanding%20of%20the%20scene%20is%20often%20limited%20to%20that%20of%20a%20monocular%20detection%20network.%20Moreover,%20objects%20in%20the%20overlap%20region%20are%20often%20largely%20occluded%20or%20suffer%20from%20deformation%20due%20to%20camera%20distortion,%20causing%20a%20domain%20shift.%20To%20mitigate%20this%20issue,%20we%20propose%20using%20the%20following%20two%20main%20modules:%20(1)%20Stereo%20Disparity%20Estimation%20for%20Weak%20Depth%20Supervision%20and%20(2)%20Adversarial%20Overlap%20Region%20Discriminator.%20The%20former%20utilizes%20the%20traditional%20stereo%20disparity%20estimation%20method%20to%20obtain%20reliable%20disparity%20information%20from%20the%20overlap%20region.%20Given%20the%20disparity%20estimates%20as%20supervision,%20we%20propose%20regularizing%20the%20network%20to%20fully%20utilize%20the%20geometric%20potential%20of%20binocular%20images%20and%20improve%20the%20overall%20detection%20accuracy%20accordingly.%20Further,%20the%20latter%20module%20minimizes%20the%20representational%20gap%20between%20non-overlap%20and%20overlapping%20regions.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20with%20the%20nuScenes%20large-scale%20multi-view%203D%20object%20detection%20data.%20Our%20experiments%20show%20that%20our%20proposed%20method%20outperforms%20current%20state-of-the-art%20models,%20i.e.,%20DETR3D%20and%20BEVDet." class="btn btn-sm z-depth-0" role="button">Abs</a> </div> </div> </div></li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%68%6D%6F%6F%6E%39%36@%6B%6F%72%65%61.%61%63.%6B%72" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=HhvS9d4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/hahamoon96" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://github.com/moonseokha" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> </div> <div class="contact-note">You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Seokha Moon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>